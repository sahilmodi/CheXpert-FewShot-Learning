{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff17d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from chexpert_specific.model.dataloader import ChexpertDataset\n",
    "import learn2learn as l2l\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import PIL.Image as Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be4f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexpertDataset(Dataset):\n",
    "    def __init__(self, csv_path: Path, split: str) -> None:\n",
    "        super(ChexpertDataset, self).__init__()\n",
    "        self.data_path = Path(csv_path).parent\n",
    "        self.annotations = pd.read_csv(csv_path).fillna(0)\n",
    "        self.train_annotations = None\n",
    "        self.split = split\n",
    "        self.transforms = None\n",
    "        self.height, self.width = 224, 224\n",
    "        self.transforms = transforms.Compose([\n",
    "                transforms.Resize((self.height, self.width)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(128, 64),\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Lambda(lambda x: transforms.functional.equalize(x)),\n",
    "                transforms.ToTensor(),\n",
    "        ])\n",
    "        if split == \"train\":\n",
    "            # assert cfg.DATA.BATCH_SIZE <= cfg.DATA.LABELED_SIZE, \"Batch size must be smaller than train size.\"\n",
    "            self.annotations = self.annotations.sample(frac=1).reset_index(drop=True)\n",
    "            # self.train_annotations = self.annotations[:cfg.DATA.LABELED_SIZE]\n",
    "            self.train_annotations = self.annotations[:1000]\n",
    "            self.transforms = transforms.Compose([\n",
    "                self.transforms,\n",
    "                transforms.RandomAffine(\n",
    "                    degrees=(-15, 15),\n",
    "                    translate=(0.05, 0.05),\n",
    "                    scale=(0.95, 1.05)\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.annotations.shape[0] if self.split != 'train' else self.train_annotations.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> None:\n",
    "        annotations = self.annotations if self.split != 'train' else self.train_annotations\n",
    "        annotation = annotations.iloc[index]\n",
    "        image = Image.open(self.data_path.parent / annotation['Path'])\n",
    "        classes = annotation[['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']].values.astype(\"float32\")\n",
    "        classes = torch.sum(torch.pow(2, torch.arange(5)) * classes)\n",
    "        data = self.transforms(image)\n",
    "        return data.repeat(3, 1, 1), classes # torch.from_numpy(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce0ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = '/home/smodi9/CheXpert-v1.0-small/'\n",
    "ds_path = Path(path_)\n",
    "split = 'train'\n",
    "\n",
    "dataset = ChexpertDataset(ds_path / f\"{split}.csv\", split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8788a164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ChexpertDataset"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de4d4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_l2l = l2l.data.MetaDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae070f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=3\n",
    "shots=1\n",
    "\n",
    "train_tasks = l2l.data.TaskDataset(dataset_l2l,\n",
    "                                   task_transforms=[\n",
    "                                         l2l.data.transforms.NWays(dataset_l2l, ways),\n",
    "                                         l2l.data.transforms.KShots(dataset_l2l, 2*shots),\n",
    "                                         l2l.data.transforms.LoadData(dataset_l2l),\n",
    "#                                          l2l.data.transforms.RemapLabels(dataset_l2l),\n",
    "#                                          l2l.data.transforms.ConsecutiveLabels(dataset_l2l),\n",
    "                                    ],\n",
    "                                    num_tasks=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0de908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3252edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3808a095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_task[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f44a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_task[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c792857b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11., 11.,  0.,  0., 20., 20.], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_task[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cd37931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=False)\n",
    "        self.backbone.fc = nn.Linear(512, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310c4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "meta_model = l2l.algorithms.MAML(model, lr=0.01)\n",
    "opt = optim.Adam(meta_model.parameters(), lr=0.005)\n",
    "loss_func = nn.NLLLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43d367e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=1000\n",
    "device=torch.device(\"cpu\")\n",
    "tps=32\n",
    "fas=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1266ace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1)\n",
    "    acc = (predictions == targets).sum().float()\n",
    "    acc /= len(targets)\n",
    "    return acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dc94306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.,  9., 11., 11., 19., 19.], dtype=torch.float64)\n",
      "torch.float32\n",
      "torch.int64\n",
      "torch.float32\n",
      "torch.float64\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 9 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c36452416c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptation_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlearner_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madaptation_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptation_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data02/arjung2/torch181_cuda11/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data02/arjung2/torch181_cuda11/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data02/arjung2/torch181_cuda11/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 9 is out of bounds."
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    iteration_error = 0.0\n",
    "    iteration_acc = 0.0\n",
    "    for _ in range(tps):\n",
    "        learner = meta_model.clone()\n",
    "        try:\n",
    "            train_task = train_tasks.sample()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        data, labels = train_task\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        print(labels)\n",
    "\n",
    "        # Separate data into adaptation/evalutation sets\n",
    "        adaptation_indices = np.zeros(data.size(0), dtype=bool)\n",
    "        adaptation_indices[np.arange(shots*ways) * 2] = True\n",
    "        evaluation_indices = torch.from_numpy(~adaptation_indices)\n",
    "        adaptation_indices = torch.from_numpy(adaptation_indices)\n",
    "        adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n",
    "        evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n",
    "\n",
    "        # Fast Adaptation\n",
    "        for step in range(fas):\n",
    "            print(adaptation_data.dtype)\n",
    "            print(adaptation_data.type(torch.LongTensor).dtype)\n",
    "            print(learner(adaptation_data).dtype)\n",
    "            print(adaptation_labels.dtype)\n",
    "            learner_ = learner(adaptation_data.type(torch.float32))\n",
    "            train_error = loss_func(learner_, adaptation_labels.long())\n",
    "            learner.adapt(train_error)\n",
    "\n",
    "        # Compute validation loss\n",
    "        predictions = learner(evaluation_data)\n",
    "        valid_error = loss_func(predictions, evaluation_labels)\n",
    "        valid_error /= len(evaluation_data)\n",
    "        valid_accuracy = accuracy(predictions, evaluation_labels)\n",
    "        iteration_error += valid_error\n",
    "        iteration_acc += valid_accuracy\n",
    "\n",
    "    iteration_error /= tps\n",
    "    iteration_acc /= tps\n",
    "    print('Loss : {:.3f} Acc : {:.3f}'.format(iteration_error, iteration_acc))\n",
    "\n",
    "    # Take the meta-learning step\n",
    "    opt.zero_grad()\n",
    "    iteration_error.backward()\n",
    "    opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
